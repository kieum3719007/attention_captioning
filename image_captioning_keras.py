# -*- coding: utf-8 -*-
"""image_captioning_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vCGOLhHRSBAH14U-uOlLbq5fOF6jRVHL

#Mount to google drive
"""

from google.colab import drive
drive.mount("/gdrive")

"""#Unzip Flickr8k data. Just run once"""

#unzip
!unzip /gdrive/MyDrive/image_captioning/data/Flickr8k_Dataset.zip -d /gdrive/MyDrive/image_captioning/data/
!unzip /gdrive/MyDrive/image_captioning/data/Flickr8k_text.zip -d /gdrive/MyDrive/image_captioning/data/

"""#Change data from Drive to local work place"""

# Commented out IPython magic to ensure Python compatibility.
#copy từ drive qua thư mục local để train nhanh hơn

WORKPLACE = "/image_captioning"
import os.path as osp
import os

if (not osp.exists(WORKPLACE)):
    os.mkdir(WORKPLACE)

# %cd {WORKPLACE}
#Copy data Flickr8k
# %cp -r /gdrive/MyDrive/image_captioning/data ./
# %cp -r /gdrive/MyDrive/image_captioning/data/vocab.txt ./

"""#Dataset and Tokenizer"""

!pip install tensorflow-text
from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab
import tensorflow_text as text
import tensorflow as tf
import re
import numpy as np
import matplotlib.pyplot as plt

# Ở đây dùng bert tokenizer của tensorflow, đây là một thuật toán subword tokenizer
# Reference: https://www.tensorflow.org/text/guide/subwords_tokenizer
# Ở đây set vocbulary size là tối đa 8000, nhưng thực tế  train với dữ liệu hiện có
# thì chỉ có 2756. config vocabulary size là 2756.
# ở đây vocabulary size là số lượng subword, chứ không phải là số lượng word. (xem file vocab.txt để thấy được các subwords)
bert_tokenizer_params=dict(lower_case=True)
reserved_tokens=["[PAD]", "[UNK]", "[START]", "[END]"]
bert_vocab_args = dict(
    # The target vocabulary size
    vocab_size = 8000,
    # Reserved tokens that must be included in the vocabulary
    reserved_tokens=reserved_tokens,
    # Arguments for `text.BertTokenizer`
    bert_tokenizer_params=bert_tokenizer_params,
    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={},
)

VOCAB_PATH = "./vocab.txt"
tokenizer = text.BertTokenizer(VOCAB_PATH, **bert_tokenizer_params)

#Helper functions giúp cho việc load dataset
import tensorflow as tf
import cv2
from tensorflow.keras.preprocessing.sequence import pad_sequences
def readlines(path):
    with open(path, "r") as reader:
        lines = reader.readlines()
    return lines

def exist_image(image_dir, fn):
    path = os.path.join(image_dir, fn)
    return os.path.exists(path)


def read_raw_data(image_dir, path):
    """
    Read raw data including filename and captioning text

    Arguments:
    ----------
        path -> str: path to data file

    Return
    ------
    Numpy array of filename and cationing text
    """

    lines = readlines(path)
    data = [process_line(line) for line in lines]
    data = list(filter(lambda x: (x != None) and (exist_image(image_dir, x[0])), data))
    data = np.stack(data, axis=0)
    return data


def process_line(line: str):
    try:
        line = line.replace("\n", "")
        filename, text = re.split("#[0-9]+\s+", line)
    except:
        print(f"[Waring]Kipped error line: {line}")
        return None
    return [filename, text]

def read_image(path):
    return plt.imread(path).astype(np.float32)

def resize_image(image, size):
    return cv2.resize(image, size)

def normalize_image(image):
    return image/128 - 1 

def preproccess_image(path, size):
    image = read_image(path)
    image = resize_image(image, size)
    image = normalize_image(image)
    return image

def preproccess_text(sentence, max_len, tokenizer, start_tok, end_tok, padding_token=0):

    #tokenize
    sequence = tokenizer.tokenize(sentence).merge_dims(-2, -1)[0]  
    #add start token and end token
    sequence = tf.concat([start_tok, sequence, end_tok], axis=0)
    #padding
    sequence = pad_sequences([sequence],
                             maxlen=max_len,
                             padding="post", truncating="post",
                             value=padding_token)[0]
    return sequence

reserved_tokens=["[PAD]", "[UNK]", "[START]", "[END]"]      #Bốn token này là 4 token đặc biệt trong vocabulary
                                                            #[PAD]: thêm vào câu nếu câu không đủ độ dài
                                                            #[UNK]: là những từ mà không nằm trong bộ từ vựng
                                                            #[START], [END]: lần lượt là <Start of sequence(sos)> và <End of sequence(eos)> 
START = tf.argmax(tf.constant(reserved_tokens) == "[START]")
END = tf.argmax(tf.constant(reserved_tokens) == "[END]")
PAD = tf.argmax(tf.constant(reserved_tokens) == "[PAD]")


class DatasetLoader:
    """
    Đọc data và preprocess.
    Ban đầu, read data từ file: Flickr_8k.lemma.token.txt, có dạng: (image_path, description)

    Preprocess image: read image từ image_path --> thực hiện process input giống với ViT -->resize 
    Preprocess text: tokenize  --> thêm [START], [END] --> chuẩn hóa độ dài chuỗi
    """
    def __init__(self,
                 batch_size,
                 root,
                 image_dir,
                 image_size,
                 text_fn,
                 max_sequence_length,
                 tokenizer):
        
        self.batch_size = batch_size
        self.root_dir = root
        self.image_dir = image_dir
        self.image_size = image_size
        self.text_path = text_fn
        self.max_sequence_length = max_sequence_length
        self.tokenizer = tokenizer


    def transform_data(self, record):
        fn, text = record
        path = os.path.join(self.root_dir, self.image_dir, fn)
        image = preproccess_image(path, self.image_size)
        sequence = preproccess_text(text,
                                    self.max_sequence_length,
                                    self.tokenizer,
                                    tf.expand_dims(START, axis=0),
                                    tf.expand_dims(END, axis=0))
        return (image, sequence)

    def load_dataset(self):
        raw_data = read_raw_data(os.path.join(self.root_dir, self.image_dir),
                                 os.path.join(self.root_dir, self.text_path))
        
        def generator():
            for record in raw_data:
                yield self.transform_data(record)

        dataset = tf.data.Dataset.from_generator(generator=generator, output_types=(tf.dtypes.float32, tf.dtypes.int16))
        
        dataset = dataset.shuffle(32, reshuffle_each_iteration=True).batch(self.batch_size, drop_remainder=True)        
        return dataset

"""#Install ViT-Keras and other requirenments
Ở phần này đơn giản là cài ViT-keras.
Sau đó load model với tùy chọn pretrained là True.
Cuối cùng là cắt bỏ lớp cuối cùng của nó.
"""

!pip install vit-keras
!pip install tensorflow-addons

from vit_keras import vit, utils
vit_model = vit.vit_b32(
    image_size=224,
    activation='sigmoid',
    pretrained=True,
    include_top=False,
    pretrained_top=False
)

from tensorflow.keras.layers import Input, Layer, Dense, Dropout, Embedding, MultiHeadAttention, LayerNormalization
from tensorflow.keras import Model, Sequential
def remove_token_layers(vit_model):
  """Remove the last layer of model"""
  vit_model.layers.pop()
  model = Model(vit_model.input, vit_model.layers[-2].output)
  return model

"""#Models
  Reference: https://keras.io/examples/nlp/neural_machine_translation_with_transformer/


  https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb?authuser=1#scrollTo=d5_d5-PLQXwY
"""

class PositionalEmbedding(Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super(PositionalEmbedding, self).__init__(**kwargs)
        self.token_embeddings = Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)

class MyTransformerDecoder(Layer):
    def __init__(self, embed_dim, num_heads, latent_dim, **kwargs):
        super(MyTransformerDecoder, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.attention_1 = MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.attention_2 = MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = Sequential(
            [Dense(latent_dim, activation="relu"), Dense(embed_dim),]
        )
        self.layernorm_1 = LayerNormalization()
        self.layernorm_2 = LayerNormalization()
        self.layernorm_3 = LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, mask=None):
        causal_mask = self.get_causal_attention_mask(inputs)
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype="int32")
            padding_mask = tf.minimum(padding_mask, causal_mask)
        else:
            padding_mask = causal_mask
        attention_output_1 = self.attention_1(
            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        proj_output = self.dense_proj(out_2)
        return self.layernorm_3(out_2 + proj_output)

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)

def get_vit_model(model_name, image_size):
  if (model_name == "vit_b32"):
    model =  vit.vit_b32( image_size=image_size,
                          activation='sigmoid',
                          pretrained=True,
                          include_top=False,
                          pretrained_top=False)
    
    return remove_token_layers(model)

def captioning_model(
    image_size,
    image_input_shape,
    text_input_shape,
    sequence_length,
    embedding_dim,
    encoder_output_dim,
    vocabulary_size, 
    num_heads,
    vit_model_name,
    latent_dim):
  
  image_inputs = Input(shape=image_input_shape, dtype="float32", name="encoder_inputs")

  vit_encoder = get_vit_model(vit_model_name, image_size[0])
  x = vit_encoder(image_inputs)
  encoder = Model(image_inputs, x, name="ViT_encoder")
  encoder_outputs = encoder(image_inputs)

  decoder_inputs = Input(shape=text_input_shape, dtype="int64", name="decoder_inputs")
  encoded_seq_inputs = Input(shape=(*text_input_shape, encoder_output_dim), name="encoder_output_inputs")
  mask = tf.not_equal(decoder_inputs, 0)
  mask = tf.cast(mask, tf.float32)
  x = PositionalEmbedding(sequence_length, vocabulary_size, embedding_dim)(decoder_inputs)

  for _ in range(12):
    x = MyTransformerDecoder(embedding_dim, num_heads, latent_dim)(x, encoded_seq_inputs, mask=mask)

  decoder_outputs = Dense(vocabulary_size, activation="softmax")(x)
  decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name="Transformer_decoder")

  decoder_output = decoder([decoder_inputs, encoder_outputs])

  model = Model([image_inputs, decoder_inputs], decoder_output, name="Transformer_Captioning")
  model.summary()
  return model

"""#Config
- Learning rate, loss thì tham khảo theo: https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb?authuser=1#scrollTo=d5_d5-PLQXwY
"""

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super(CustomSchedule, self).__init__()

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)

    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)
LOSS = loss_function
EPOCH = 20

DATA_PATH = {
    "root": "./data",
    "image_dir": "Flicker8k_Dataset",
    "text_fn": "Flickr8k.lemma.token.txt"
}

IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
EMBEDDING_DIM = 256
IMAGE_INPUT_SHAPE = (*IMAGE_SIZE, 3)
SEQUENCE_LENGTH = 50
TEXT_INPUT_SHAPE = (SEQUENCE_LENGTH,)
VOCABULARY_SIZE = 2756
NUM_HEADS = 5
VIT_NAME = "vit_b32"
VIT_DIM = 768

learning_rate = CustomSchedule(EMBEDDING_DIM)

OPTIMIZER = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)
MODEL_CONFIG = {
    "image_size": IMAGE_SIZE,
    "image_input_shape": IMAGE_INPUT_SHAPE,
    "text_input_shape": TEXT_INPUT_SHAPE,
    "sequence_length": SEQUENCE_LENGTH,
    "embedding_dim": EMBEDDING_DIM,
    "encoder_output_dim":VIT_DIM,
    "vocabulary_size": VOCABULARY_SIZE, 
    "num_heads": NUM_HEADS,
    "vit_model_name": VIT_NAME,
    "latent_dim": VIT_DIM
}

"""#Checkpoints"""

checkpoint_path = "./checkpoints/train"

ckpt = tf.train.Checkpoint(model=model,
                           optimizer=OPTIMIZER)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

# if a checkpoint exists, restore the latest checkpoint.
if ckpt_manager.latest_checkpoint:
  ckpt.restore(ckpt_manager.latest_checkpoint)
  print('Latest checkpoint restored!!')

"""#Training"""

dataset = DatasetLoader(
    batch_size = BATCH_SIZE,
    image_size = IMAGE_SIZE,
    max_sequence_length = SEQUENCE_LENGTH + 1,
    tokenizer = tokenizer,
    **DATA_PATH
).load_dataset()
model = captioning_model(**MODEL_CONFIG)
model.compile(OPTIMIZER, LOSS)

# Commented out IPython magic to ensure Python compatibility.
train_step_signature = [
    tf.TensorSpec(shape=(BATCH_SIZE, *IMAGE_INPUT_SHAPE), dtype=tf.float32),
    tf.TensorSpec(shape=(BATCH_SIZE, SEQUENCE_LENGTH + 1), dtype=tf.int16),
]

@tf.function(input_signature=train_step_signature)
def train_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]
    with tf.GradientTape() as tape:
        predictions = model([inp, tar_inp],
                                    training = True)
        loss = LOSS(tar_real, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    OPTIMIZER.apply_gradients(zip(gradients, model.trainable_variables))

    return loss

for epoch in range(EPOCH):
    print("\nStart of epoch %d" % (epoch + 1,))
    for step, (inp,tar) in enumerate(dataset):
        loss_value = train_step(inp,tar)
        if step % 100 == 0:
            print(
                "Training loss (for one batch) at step %d: %.4f"
#                 % (step, float(loss_value))
            )

    if (epoch + 1) % 5 == 0:
        ckpt_save_path = ckpt_manager.save()
        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')

!cp ./checkpoints /gdrive/MyDrive/image_captioning

model.save("/gdrive/MyDrive/image_captioning/saved_model")